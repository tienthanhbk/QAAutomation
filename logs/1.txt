Train on 7550 samples, validate on 1803 samples
Epoch 1/1
  80/7550 [..............................] - ETA: 27:41 - loss: 1.3182
 160/7550 [..............................] - ETA: 18:43 - loss: 1.1617
 240/7550 [..............................] - ETA: 15:42 - loss: 1.1813
 320/7550 [>.............................] - ETA: 14:07 - loss: 1.2057
 400/7550 [>.............................] - ETA: 13:09 - loss: 1.2636
 480/7550 [>.............................] - ETA: 12:24 - loss: 1.2895
 560/7550 [=>............................] - ETA: 11:52 - loss: 1.2672
 640/7550 [=>............................] - ETA: 11:24 - loss: 1.2383
 720/7550 [=>............................] - ETA: 11:02 - loss: 1.2212
 800/7550 [==>...........................] - ETA: 10:45 - loss: 1.2159
 880/7550 [==>...........................] - ETA: 10:29 - loss: 1.2398
 960/7550 [==>...........................] - ETA: 10:13 - loss: 1.2633
1040/7550 [===>..........................] - ETA: 9:59 - loss: 1.2372
1120/7550 [===>..........................] - ETA: 9:46 - loss: 1.2409
1200/7550 [===>..........................] - ETA: 9:33 - loss: 1.2364
1280/7550 [====>.........................] - ETA: 9:22 - loss: 1.2146
1360/7550 [====>.........................] - ETA: 9:10 - loss: 1.2228
1440/7550 [====>.........................] - ETA: 9:00 - loss: 1.2329
1520/7550 [=====>........................] - ETA: 8:50 - loss: 1.2349
1600/7550 [=====>........................] - ETA: 8:40 - loss: 1.2460
1680/7550 [=====>........................] - ETA: 8:30 - loss: 1.2556
1760/7550 [=====>........................] - ETA: 8:21 - loss: 1.2536
1840/7550 [======>.......................] - ETA: 8:12 - loss: 1.2560
1920/7550 [======>.......................] - ETA: 8:04 - loss: 1.2613
2000/7550 [======>.......................] - ETA: 7:55 - loss: 1.2523
2080/7550 [=======>......................] - ETA: 7:47 - loss: 1.2519
2160/7550 [=======>......................] - ETA: 7:38 - loss: 1.2588
2240/7550 [=======>......................] - ETA: 7:30 - loss: 1.2550
2320/7550 [========>.....................] - ETA: 7:22 - loss: 1.2587
2400/7550 [========>.....................] - ETA: 7:14 - loss: 1.2489
2480/7550 [========>.....................] - ETA: 7:06 - loss: 1.2423
2560/7550 [=========>....................] - ETA: 6:59 - loss: 1.2411
2640/7550 [=========>....................] - ETA: 6:51 - loss: 1.2416
2720/7550 [=========>....................] - ETA: 6:43 - loss: 1.2370
2800/7550 [==========>...................] - ETA: 6:36 - loss: 1.2355
2880/7550 [==========>...................] - ETA: 6:28 - loss: 1.2367
2960/7550 [==========>...................] - ETA: 6:21 - loss: 1.2406
3040/7550 [===========>..................] - ETA: 6:14 - loss: 1.2410
3120/7550 [===========>..................] - ETA: 6:07 - loss: 1.2365
3200/7550 [===========>..................] - ETA: 6:00 - loss: 1.2293
3280/7550 [============>.................] - ETA: 5:52 - loss: 1.2267
3360/7550 [============>.................] - ETA: 5:45 - loss: 1.2182
3440/7550 [============>.................] - ETA: 5:38 - loss: 1.2142
3520/7550 [============>.................] - ETA: 5:35 - loss: 1.2160
3600/7550 [=============>................] - ETA: 5:29 - loss: 1.2127
3680/7550 [=============>................] - ETA: 5:23 - loss: 1.2113
3760/7550 [=============>................] - ETA: 5:16 - loss: 1.2112
3840/7550 [==============>...............] - ETA: 5:10 - loss: 1.2032
3920/7550 [==============>...............] - ETA: 5:05 - loss: 1.2110
4000/7550 [==============>...............] - ETA: 4:58 - loss: 1.2095
4080/7550 [===============>..............] - ETA: 4:52 - loss: 1.2150
4160/7550 [===============>..............] - ETA: 4:45 - loss: 1.2150
4240/7550 [===============>..............] - ETA: 4:38 - loss: 1.2114
4320/7550 [================>.............] - ETA: 4:31 - loss: 1.2189
4400/7550 [================>.............] - ETA: 4:24 - loss: 1.2141
4480/7550 [================>.............] - ETA: 4:17 - loss: 1.2159
4560/7550 [=================>............] - ETA: 4:12 - loss: 1.2194
4640/7550 [=================>............] - ETA: 4:06 - loss: 1.2220
4720/7550 [=================>............] - ETA: 3:59 - loss: 1.2200
4800/7550 [==================>...........] - ETA: 3:52 - loss: 1.2246
4880/7550 [==================>...........] - ETA: 3:46 - loss: 1.2237
4960/7550 [==================>...........] - ETA: 3:40 - loss: 1.2196
5040/7550 [===================>..........] - ETA: 3:34 - loss: 1.2202
5120/7550 [===================>..........] - ETA: 3:28 - loss: 1.2199
5200/7550 [===================>..........] - ETA: 3:22 - loss: 1.2251
5280/7550 [===================>..........] - ETA: 3:15 - loss: 1.2227
5360/7550 [====================>.........] - ETA: 3:09 - loss: 1.2198
5440/7550 [====================>.........] - ETA: 3:02 - loss: 1.2170
5520/7550 [====================>.........] - ETA: 2:55 - loss: 1.2152
5600/7550 [=====================>........] - ETA: 2:48 - loss: 1.2102
5680/7550 [=====================>........] - ETA: 2:42 - loss: 1.2072
5760/7550 [=====================>........] - ETA: 2:35 - loss: 1.2055
5840/7550 [======================>.......] - ETA: 2:28 - loss: 1.2050
5920/7550 [======================>.......] - ETA: 2:21 - loss: 1.2067
6000/7550 [======================>.......] - ETA: 2:15 - loss: 1.2113
6080/7550 [=======================>......] - ETA: 2:08 - loss: 1.2097
6160/7550 [=======================>......] - ETA: 2:01 - loss: 1.2066
6240/7550 [=======================>......] - ETA: 1:54 - loss: 1.2063
6320/7550 [========================>.....] - ETA: 1:47 - loss: 1.2065
6400/7550 [========================>.....] - ETA: 1:41 - loss: 1.2047
6480/7550 [========================>.....] - ETA: 1:34 - loss: 1.2071
6560/7550 [=========================>....] - ETA: 1:27 - loss: 1.2069
6640/7550 [=========================>....] - ETA: 1:20 - loss: 1.2074
6720/7550 [=========================>....] - ETA: 1:13 - loss: 1.2044
6800/7550 [==========================>...] - ETA: 1:06 - loss: 1.2061
6880/7550 [==========================>...] - ETA: 59s - loss: 1.2119
6960/7550 [==========================>...] - ETA: 52s - loss: 1.2114
7040/7550 [==========================>...] - ETA: 45s - loss: 1.2155
7120/7550 [===========================>..] - ETA: 38s - loss: 1.2134
7200/7550 [===========================>..] - ETA: 31s - loss: 1.2162
7280/7550 [===========================>..] - ETA: 23s - loss: 1.2198
7360/7550 [============================>.] - ETA: 16s - loss: 1.2206
7440/7550 [============================>.] - ETA: 9s - loss: 1.2179
7520/7550 [============================>.] - ETA: 2s - loss: 1.2159
7550/7550 [==============================] - 701s 93ms/step - loss: 1.2170 - val_loss: 0.7496
val MRR 0.661667; val MAP 0.275702
Epoch 00001: map improved from -inf to 0.27570, saving model to model_CuDNNimprovement-01-0.28.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
question_base (InputLayer)      (None, 150)          0
__________________________________________________________________________________________________
answer (InputLayer)             (None, 150)          0
__________________________________________________________________________________________________
bi_lstm (Model)                 (None, 1)            177600529   question_base[0][0]
                                                                 answer[0][0]
==================================================================================================
Total params: 177,600,529
Trainable params: 177,598,929
Non-trainable params: 1,600
__________________________________________________________________________________________________

PATH_DATA_TRAIN = 'data/train.txt'
PATH_DATA_DEV = 'data/dev.txt'
PATH_DATA_TEST = 'data/lstm/test_lstm.txt'
PATH_WORD_VECTOR = 'data/lstm/vectors_baomoi.txt'
PATH_VOCAB = 'data/lstm/vocab_baomoi.txt'
wordvector_dims = 400

qa_embedding = Embedding(
        input_dim=vocab_size + 1, input_length=None, output_dim=weights.shape[1], mask_zero=True, weights=[weights])

bi_lstm1 = Bidirectional(
    LSTM(units=200, return_sequences=False))
bi_lstm2 = Bidirectional(
    LSTM(units=200, return_sequences=False))

training_model.fit(
    [questions_origin, question_related],
    Y,
    epochs=1,
    batch_size=80,
    validation_data=([q_origin_dev_eb, q_related_dev_eb], l_dev),
    verbose=1,
    callbacks=callback_list
)
