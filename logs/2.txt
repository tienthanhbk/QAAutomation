Train on 7550 samples, validate on 1803 samples
Epoch 1/1
  80/7550 [..............................] - ETA: 16:57 - loss: 1.6003
 160/7550 [..............................] - ETA: 11:15 - loss: 1.5558
 240/7550 [..............................] - ETA: 9:25 - loss: 1.5568
 320/7550 [>.............................] - ETA: 8:55 - loss: 1.5307
 400/7550 [>.............................] - ETA: 8:54 - loss: 1.5332
 480/7550 [>.............................] - ETA: 8:43 - loss: 1.4908
 560/7550 [=>............................] - ETA: 8:10 - loss: 1.5243
 640/7550 [=>............................] - ETA: 7:44 - loss: 1.5024
 720/7550 [=>............................] - ETA: 7:26 - loss: 1.5004
 800/7550 [==>...........................] - ETA: 7:13 - loss: 1.5134
 880/7550 [==>...........................] - ETA: 6:54 - loss: 1.5063
 960/7550 [==>...........................] - ETA: 6:36 - loss: 1.5194
1040/7550 [===>..........................] - ETA: 6:22 - loss: 1.5049
1120/7550 [===>..........................] - ETA: 6:07 - loss: 1.5051
1200/7550 [===>..........................] - ETA: 6:01 - loss: 1.5096
1280/7550 [====>.........................] - ETA: 5:52 - loss: 1.5260
1360/7550 [====>.........................] - ETA: 5:44 - loss: 1.5155
1440/7550 [====>.........................] - ETA: 5:36 - loss: 1.5217
1520/7550 [=====>........................] - ETA: 5:28 - loss: 1.5353
1600/7550 [=====>........................] - ETA: 5:22 - loss: 1.5352
1680/7550 [=====>........................] - ETA: 5:13 - loss: 1.5352
1760/7550 [=====>........................] - ETA: 5:04 - loss: 1.5376
1840/7550 [======>.......................] - ETA: 4:56 - loss: 1.5272
1920/7550 [======>.......................] - ETA: 4:48 - loss: 1.5257
2000/7550 [======>.......................] - ETA: 4:40 - loss: 1.5258
2080/7550 [=======>......................] - ETA: 4:33 - loss: 1.5307
2160/7550 [=======>......................] - ETA: 4:27 - loss: 1.5386
2240/7550 [=======>......................] - ETA: 4:21 - loss: 1.5378
2320/7550 [========>.....................] - ETA: 4:16 - loss: 1.5343
2400/7550 [========>.....................] - ETA: 4:10 - loss: 1.5241
2480/7550 [========>.....................] - ETA: 4:04 - loss: 1.5169
2560/7550 [=========>....................] - ETA: 3:58 - loss: 1.5166
2640/7550 [=========>....................] - ETA: 3:54 - loss: 1.5157
2720/7550 [=========>....................] - ETA: 3:50 - loss: 1.5214
2800/7550 [==========>...................] - ETA: 3:46 - loss: 1.5087
2880/7550 [==========>...................] - ETA: 3:43 - loss: 1.5079
2960/7550 [==========>...................] - ETA: 3:38 - loss: 1.5052
3040/7550 [===========>..................] - ETA: 3:33 - loss: 1.5056
3120/7550 [===========>..................] - ETA: 3:27 - loss: 1.5007
3200/7550 [===========>..................] - ETA: 3:22 - loss: 1.4946
3280/7550 [============>.................] - ETA: 3:17 - loss: 1.4948
3360/7550 [============>.................] - ETA: 3:13 - loss: 1.4962
3440/7550 [============>.................] - ETA: 3:08 - loss: 1.5034
3520/7550 [============>.................] - ETA: 3:03 - loss: 1.5024
3600/7550 [=============>................] - ETA: 2:58 - loss: 1.5015
3680/7550 [=============>................] - ETA: 2:54 - loss: 1.5045
3760/7550 [=============>................] - ETA: 2:50 - loss: 1.5070
3840/7550 [==============>...............] - ETA: 2:45 - loss: 1.5065
3920/7550 [==============>...............] - ETA: 2:41 - loss: 1.5062
4000/7550 [==============>...............] - ETA: 2:37 - loss: 1.5017
4080/7550 [===============>..............] - ETA: 2:33 - loss: 1.5090
4160/7550 [===============>..............] - ETA: 2:29 - loss: 1.5054
4240/7550 [===============>..............] - ETA: 2:25 - loss: 1.5047
4320/7550 [================>.............] - ETA: 2:21 - loss: 1.5026
4400/7550 [================>.............] - ETA: 2:17 - loss: 1.4973
4480/7550 [================>.............] - ETA: 2:13 - loss: 1.4982
4560/7550 [=================>............] - ETA: 2:09 - loss: 1.4978
4640/7550 [=================>............] - ETA: 2:05 - loss: 1.4971
4720/7550 [=================>............] - ETA: 2:01 - loss: 1.4926
4800/7550 [==================>...........] - ETA: 1:57 - loss: 1.4900
4880/7550 [==================>...........] - ETA: 1:53 - loss: 1.4843
4960/7550 [==================>...........] - ETA: 1:50 - loss: 1.4834
5040/7550 [===================>..........] - ETA: 1:46 - loss: 1.4835
5120/7550 [===================>..........] - ETA: 1:42 - loss: 1.4806
5200/7550 [===================>..........] - ETA: 1:39 - loss: 1.4831
5280/7550 [===================>..........] - ETA: 1:35 - loss: 1.4803
5360/7550 [====================>.........] - ETA: 1:31 - loss: 1.4745
5440/7550 [====================>.........] - ETA: 1:28 - loss: 1.4737
5520/7550 [====================>.........] - ETA: 1:24 - loss: 1.4704
5600/7550 [=====================>........] - ETA: 1:21 - loss: 1.4667
5680/7550 [=====================>........] - ETA: 1:18 - loss: 1.4690
5760/7550 [=====================>........] - ETA: 1:15 - loss: 1.4669
5840/7550 [======================>.......] - ETA: 1:11 - loss: 1.4642
5920/7550 [======================>.......] - ETA: 1:08 - loss: 1.4652
6000/7550 [======================>.......] - ETA: 1:04 - loss: 1.4611
6080/7550 [=======================>......] - ETA: 1:01 - loss: 1.4602
6160/7550 [=======================>......] - ETA: 58s - loss: 1.4602
6240/7550 [=======================>......] - ETA: 54s - loss: 1.4579
6320/7550 [========================>.....] - ETA: 51s - loss: 1.4580
6400/7550 [========================>.....] - ETA: 48s - loss: 1.4583
6480/7550 [========================>.....] - ETA: 44s - loss: 1.4552
6560/7550 [=========================>....] - ETA: 41s - loss: 1.4516
6640/7550 [=========================>....] - ETA: 37s - loss: 1.4511
6720/7550 [=========================>....] - ETA: 34s - loss: 1.4493
6800/7550 [==========================>...] - ETA: 31s - loss: 1.4491
6880/7550 [==========================>...] - ETA: 27s - loss: 1.4475
6960/7550 [==========================>...] - ETA: 24s - loss: 1.4454
7040/7550 [==========================>...] - ETA: 20s - loss: 1.4450
7120/7550 [===========================>..] - ETA: 17s - loss: 1.4457
7200/7550 [===========================>..] - ETA: 14s - loss: 1.4456
7280/7550 [===========================>..] - ETA: 11s - loss: 1.4460
7360/7550 [============================>.] - ETA: 7s - loss: 1.4471
7440/7550 [============================>.] - ETA: 4s - loss: 1.4474
7520/7550 [============================>.] - ETA: 1s - loss: 1.4470
7550/7550 [==============================] - 331s 44ms/step - loss: 1.4454 - val_loss: 0.8434
val MRR 0.406530; val MAP 0.259748
Epoch 00001: map improved from -inf to 0.25975, saving model to model_CuDNNimprovement-01-0.26.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
question_base (InputLayer)      (None, 150)          0
__________________________________________________________________________________________________
answer (InputLayer)             (None, 150)          0
__________________________________________________________________________________________________
bi_lstm (Model)                 (None, 1)            2639729     question_base[0][0]
                                                                 answer[0][0]
==================================================================================================
Total params: 2,639,729
Trainable params: 2,638,129
Non-trainable params: 1,600
__________________________________________________________________________________________________

PATH_DATA_TRAIN = 'data/train.txt'
PATH_DATA_DEV = 'data/dev.txt'
PATH_DATA_TEST = 'data/lstm/test_lstm.txt'
PATH_WORD_VECTOR = 'data/lstm/vectors.txt'
PATH_VOCAB = 'data/lstm/vocab_all.txt'
wordvector_dims = 200

qa_embedding = Embedding(
    input_dim=vocab_size + 1,
    input_length=None,
    output_dim=weights.shape[1],
    mask_zero=True,
    weights=[weights])

bi_lstm1 = Bidirectional(
    LSTM(units=200, return_sequences=False))
bi_lstm2 = Bidirectional(
    LSTM(units=200, return_sequences=False))


training_model.fit(
    [questions_origin, question_related],
    Y,
    epochs=1,
    batch_size=80,
    validation_data=([q_origin_dev_eb, q_related_dev_eb], l_dev),
    verbose=1,
    callbacks=callback_list
)
