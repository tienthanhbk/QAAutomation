Train on 7550 samples, validate on 1803 samples
Epoch 1/1
  80/7550 [..............................] - ETA: 10:25 - loss: 1.4810
 160/7550 [..............................] - ETA: 7:16 - loss: 1.2886
 240/7550 [..............................] - ETA: 6:13 - loss: 1.2576
 320/7550 [>.............................] - ETA: 5:38 - loss: 1.3308
 400/7550 [>.............................] - ETA: 5:15 - loss: 1.3302
 480/7550 [>.............................] - ETA: 4:59 - loss: 1.3310
 560/7550 [=>............................] - ETA: 4:46 - loss: 1.3247
 640/7550 [=>............................] - ETA: 4:36 - loss: 1.3382
 720/7550 [=>............................] - ETA: 4:28 - loss: 1.3645
 800/7550 [==>...........................] - ETA: 4:21 - loss: 1.3488
 880/7550 [==>...........................] - ETA: 4:14 - loss: 1.3422
 960/7550 [==>...........................] - ETA: 4:08 - loss: 1.3235
1040/7550 [===>..........................] - ETA: 4:03 - loss: 1.3374
1120/7550 [===>..........................] - ETA: 3:58 - loss: 1.3446
1200/7550 [===>..........................] - ETA: 3:53 - loss: 1.3443
1280/7550 [====>.........................] - ETA: 3:49 - loss: 1.3424
1360/7550 [====>.........................] - ETA: 3:44 - loss: 1.3455
1440/7550 [====>.........................] - ETA: 3:40 - loss: 1.3465
1520/7550 [=====>........................] - ETA: 3:36 - loss: 1.3506
1600/7550 [=====>........................] - ETA: 3:32 - loss: 1.3468
1680/7550 [=====>........................] - ETA: 3:29 - loss: 1.3658
1760/7550 [=====>........................] - ETA: 3:25 - loss: 1.3654
1840/7550 [======>.......................] - ETA: 3:21 - loss: 1.3726
1920/7550 [======>.......................] - ETA: 3:18 - loss: 1.3775
2000/7550 [======>.......................] - ETA: 3:15 - loss: 1.3742
2080/7550 [=======>......................] - ETA: 3:12 - loss: 1.3698
2160/7550 [=======>......................] - ETA: 3:08 - loss: 1.3626
2240/7550 [=======>......................] - ETA: 3:05 - loss: 1.3542
2320/7550 [========>.....................] - ETA: 3:02 - loss: 1.3412
2400/7550 [========>.....................] - ETA: 2:59 - loss: 1.3323
2480/7550 [========>.....................] - ETA: 2:56 - loss: 1.3395
2560/7550 [=========>....................] - ETA: 2:53 - loss: 1.3344
2640/7550 [=========>....................] - ETA: 2:50 - loss: 1.3344
2720/7550 [=========>....................] - ETA: 2:47 - loss: 1.3333
2800/7550 [==========>...................] - ETA: 2:44 - loss: 1.3267
2880/7550 [==========>...................] - ETA: 2:41 - loss: 1.3259
2960/7550 [==========>...................] - ETA: 2:38 - loss: 1.3270
3040/7550 [===========>..................] - ETA: 2:35 - loss: 1.3316
3120/7550 [===========>..................] - ETA: 2:32 - loss: 1.3293
3200/7550 [===========>..................] - ETA: 2:29 - loss: 1.3246
3280/7550 [============>.................] - ETA: 2:26 - loss: 1.3217
3360/7550 [============>.................] - ETA: 2:23 - loss: 1.3250
3440/7550 [============>.................] - ETA: 2:20 - loss: 1.3182
3520/7550 [============>.................] - ETA: 2:17 - loss: 1.3143
3600/7550 [=============>................] - ETA: 2:14 - loss: 1.3150
3680/7550 [=============>................] - ETA: 2:12 - loss: 1.3189
3760/7550 [=============>................] - ETA: 2:09 - loss: 1.3154
3840/7550 [==============>...............] - ETA: 2:06 - loss: 1.3141
3920/7550 [==============>...............] - ETA: 2:03 - loss: 1.3082
4000/7550 [==============>...............] - ETA: 2:00 - loss: 1.3000
4080/7550 [===============>..............] - ETA: 1:57 - loss: 1.2977
4160/7550 [===============>..............] - ETA: 1:54 - loss: 1.2934
4240/7550 [===============>..............] - ETA: 1:52 - loss: 1.2952
4320/7550 [================>.............] - ETA: 1:49 - loss: 1.2877
4400/7550 [================>.............] - ETA: 1:46 - loss: 1.2872
4480/7550 [================>.............] - ETA: 1:43 - loss: 1.2865
4560/7550 [=================>............] - ETA: 1:41 - loss: 1.2853
4640/7550 [=================>............] - ETA: 1:38 - loss: 1.2807
4720/7550 [=================>............] - ETA: 1:35 - loss: 1.2803
4800/7550 [==================>...........] - ETA: 1:32 - loss: 1.2821
4880/7550 [==================>...........] - ETA: 1:30 - loss: 1.2795
4960/7550 [==================>...........] - ETA: 1:28 - loss: 1.2851
5040/7550 [===================>..........] - ETA: 1:26 - loss: 1.2814
5120/7550 [===================>..........] - ETA: 1:24 - loss: 1.2818
5200/7550 [===================>..........] - ETA: 1:22 - loss: 1.2860
5280/7550 [===================>..........] - ETA: 1:19 - loss: 1.2812
5360/7550 [====================>.........] - ETA: 1:17 - loss: 1.2792
5440/7550 [====================>.........] - ETA: 1:14 - loss: 1.2795
5520/7550 [====================>.........] - ETA: 1:11 - loss: 1.2772
5600/7550 [=====================>........] - ETA: 1:08 - loss: 1.2758
5680/7550 [=====================>........] - ETA: 1:06 - loss: 1.2756
5760/7550 [=====================>........] - ETA: 1:03 - loss: 1.2694
5840/7550 [======================>.......] - ETA: 1:00 - loss: 1.2715
5920/7550 [======================>.......] - ETA: 57s - loss: 1.2680
6000/7550 [======================>.......] - ETA: 54s - loss: 1.2679
6080/7550 [=======================>......] - ETA: 51s - loss: 1.2657
6160/7550 [=======================>......] - ETA: 49s - loss: 1.2635
6240/7550 [=======================>......] - ETA: 46s - loss: 1.2612
6320/7550 [========================>.....] - ETA: 43s - loss: 1.2603
6400/7550 [========================>.....] - ETA: 40s - loss: 1.2613
6480/7550 [========================>.....] - ETA: 37s - loss: 1.2619
6560/7550 [=========================>....] - ETA: 34s - loss: 1.2627
6640/7550 [=========================>....] - ETA: 32s - loss: 1.2611
6720/7550 [=========================>....] - ETA: 29s - loss: 1.2599
6800/7550 [==========================>...] - ETA: 26s - loss: 1.2595
6880/7550 [==========================>...] - ETA: 23s - loss: 1.2583
6960/7550 [==========================>...] - ETA: 20s - loss: 1.2559
7040/7550 [==========================>...] - ETA: 17s - loss: 1.2585
7120/7550 [===========================>..] - ETA: 15s - loss: 1.2565
7200/7550 [===========================>..] - ETA: 12s - loss: 1.2536
7280/7550 [===========================>..] - ETA: 9s - loss: 1.2510
7360/7550 [============================>.] - ETA: 6s - loss: 1.2486
7440/7550 [============================>.] - ETA: 3s - loss: 1.2468
7520/7550 [============================>.] - ETA: 1s - loss: 1.2454
7550/7550 [==============================] - 285s 38ms/step - loss: 1.2435 - val_loss: 0.7318
val MRR 0.569702; val MAP 0.298741
Epoch 00001: map improved from -inf to 0.29874, saving model to model_CuDNNimprovement-01-0.30.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
question_base (InputLayer)      (None, 150)          0
__________________________________________________________________________________________________
answer (InputLayer)             (None, 150)          0
__________________________________________________________________________________________________
bi_lstm (Model)                 (None, 1)            1998129     question_base[0][0]
                                                                 answer[0][0]
==================================================================================================
Total params: 1,998,129
Trainable params: 1,996,529
Non-trainable params: 1,600
__________________________________________________________________________________________________

PATH_DATA_TRAIN = 'data/train.txt'
PATH_DATA_DEV = 'data/dev.txt'
PATH_DATA_TEST = 'data/lstm/test_lstm.txt'
PATH_WORD_VECTOR = 'data/lstm/vectors.txt'
PATH_VOCAB = 'data/lstm/vocab_all.txt'
wordvector_dims = 200

qa_embedding = Embedding(
    input_dim=vocab_size + 1,
    input_length=None,
    output_dim=weights.shape[1],
    mask_zero=True,
    weights=[weights])

bi_lstm = Bidirectional(
    LSTM(units=200, return_sequences=False))

training_model.fit(
    [questions_origin, question_related],
    Y,
    epochs=1,
    batch_size=80,
    validation_data=([q_origin_dev_eb, q_related_dev_eb], l_dev),
    verbose=1,
    callbacks=callback_list
)


